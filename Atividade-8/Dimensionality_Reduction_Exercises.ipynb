{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios de Redução de Dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Utilizaremos os dados de um [distribuidor por atacado Português](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers) para agrupamento. Essa base de dados tem o nome de `Wholesale_Customers_Data`.\n",
    "\n",
    "Essa base contêm os seguintes atributos:\n",
    "\n",
    "* Fresh: annual spending (m.u.) on fresh products\n",
    "* Milk: annual spending (m.u.) on milk products\n",
    "* Grocery: annual spending (m.u.) on grocery products\n",
    "* Frozen: annual spending (m.u.) on frozen products\n",
    "* Detergents_Paper: annual spending (m.u.) on detergents and paper products\n",
    "* Delicatessen: annual spending (m.u.) on delicatessen products\n",
    "* Channel: customer channel (1: hotel/restaurant/cafe or 2: retail)\n",
    "* Region: customer region (1: Lisbon, 2: Porto, 3: Other)\n",
    "\n",
    "Nessa base, os valores de gastos são dados em uma unidade arbitrária (m.u. = monetary unit, unidade monetária)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1\n",
    "\n",
    "* Importe os dados e verifique os seus tipos.\n",
    "* Remova as colunas channel e region.\n",
    "* Converta o restante para floats se necessário.\n",
    "* Copie a variável original de data (usando o método `copy`) para preservá-la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filepath = 'data/Wholesale_Customers_Data.csv'\n",
    "data = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop(???, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tipos das variáveis\n",
    "data.???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converte para floats\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça uma cópia do original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_orig = data.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2\n",
    "\n",
    "Vamos repetir o procedimento de transformação e escala da atividade anterior (Exercício 2):\n",
    "\n",
    "* Examine a correlação e viés.\n",
    "* Faça a transformação e escalonamento necessários.\n",
    "* Visualize as correlações par a par."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matriz de correlação\n",
    "corr_mat = data.???\n",
    "\n",
    "# Sete a diagonal com 0.0\n",
    "for x in range(corr_mat.shape[0]):\n",
    "    corr_mat.iloc[x,x] = ???\n",
    "    \n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retorne o id de máxima correlação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat.???.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine os atributos com viés e aplique log1p:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_columns = data.???.sort_values(ascending=False)\n",
    "log_columns = log_columns.loc[log_columns > 0.75]\n",
    "\n",
    "log_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The log transformations\n",
    "for col in log_columns.index:\n",
    "    data[col] = ???(data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos usar o `MinMaxScaler` para fazer o escalonamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "for col in data.columns:\n",
    "    data[col] = mms.fit_transform(???).squeeze() # squeeze transforma em vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos plotar as relações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "sns.set_palette('dark')\n",
    "sns.set_style('white')\n",
    "\n",
    "sns.pairplot(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3\n",
    "\n",
    "* Usando a função [pipeline](http://scikit-learn.org/stable/modules/pipeline.html) do Scikit-Learn, recrie o procedimento acima de pré-processamento (transformação e escala) usando pipeline. Para utilizar uma função de transformação externa (log1p do Numpy) você precisará da classe chamada [`FunctionTransformer`](http://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers).\n",
    "* Use o pipeline para transformar a cópia dos dados originais.\n",
    "* Compare com o resultado do exercício anterior para verificar se funcionou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Crie a transformação log do Numpy para ser utilizada no pipeline\n",
    "log_transformer = FunctionTransformer(???)\n",
    "\n",
    "# O pipeline\n",
    "estimators = [('log1p', log_transformer), ('minmaxscale', MinMaxScaler())]\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Converte a cópia dos dados originais\n",
    "data_pipe = pipeline.fit_transform(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados são idênticos. Note que podemos acrescentar qualquer modelo dentro do pipeline, como algoritmos de classificação e regressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(data_pipe, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 4\n",
    "\n",
    "* Aplique o PCA com `n_components` variando entre 1 e 5. \n",
    "* Armazene a quantidade de variância explicada (explained_variance_ratio_.sum()) para cada número de dimensões.\n",
    "* Vamos estimar a importância dos atributos gerados com np.abs(components_).sum(axis=0)/ soma de todos.\n",
    "* Plote os valores acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_list = list()\n",
    "feature_weight_list = list()\n",
    "\n",
    "# Fit a range of PCA models\n",
    "\n",
    "for n in range(???, ???):\n",
    "    \n",
    "    # Create and fit the model\n",
    "    PCAmod = PCA(n_components=???)\n",
    "    PCAmod.fit(???)\n",
    "    \n",
    "    # Store the model and variance\n",
    "    pca_list.append(pd.Series({'n':n, 'model':PCAmod,\n",
    "                               'var': PCAmod.explained_variance_ratio_.sum()}))\n",
    "    \n",
    "    # Calculate and store feature importances\n",
    "    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)\n",
    "    feature_weight_list.append(pd.DataFrame({'n':n, \n",
    "                                             'features': data.columns,\n",
    "                                             'values':abs_feature_values/abs_feature_values.sum()}))\n",
    "    \n",
    "pca_df = pd.concat(pca_list, axis=1).T.set_index('n')\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma tabela para verificar os resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = (pd.concat(feature_weight_list)\n",
    "               .pivot(index='n', columns='features', values='values'))\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E vamos plotar os resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "\n",
    "ax = pca_df['var'].plot(kind='bar')\n",
    "\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Percent explained variance',\n",
    "       title='Explained Variance vs Dimensions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E da importância dos atributos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = features_df.plot(kind='bar')\n",
    "\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Relative importance',\n",
    "       title='Feature importance vs Dimensions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 5\n",
    "\n",
    "Vamos verificar como a acurácia de nosso modelo pode mudar se incluírmos um `PCA` no nosso pipeline de processamento. Vamos criar um pipeline com os seguintes passos:\n",
    "<ol>\n",
    "  <li>Escalonar</li>\n",
    "  <li>`PCA(n_components=n)`</li>\n",
    "  <li>`LogisticRegression`</li>\n",
    "</ol>\n",
    "\n",
    "* Para isso utilizaremos a base de dados Human Activity.\n",
    "* Escreva uma função que pega um valor  `n`  e constrói o pipeline acima, faça a predição da coluna  \"Activity\" em um StratifiedShuffleSplit de 5 pastas (fold), retorne a acurácia média da base de teste.\n",
    "* Chame a função acima para vários valores de n.\n",
    "* Plote a acurácia média pela dimensão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = 'data/Human_Activity_Recognition_Using_Smartphones_Data.csv'\n",
    "data2 = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X = data2.drop('Activity', axis=1)\n",
    "y = data2.Activity\n",
    "sss = StratifiedShuffleSplit(n_splits=???, random_state=42)\n",
    "\n",
    "def get_avg_score(n):\n",
    "    pipe = [\n",
    "        ('scaler', ???),\n",
    "        ('pca', ???),\n",
    "        ('estimator', ???)\n",
    "    ]\n",
    "    pipe = Pipeline(pipe)\n",
    "    scores = []\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        pipe.fit(???, ???)\n",
    "        scores.append(accuracy_score(???, pipe.predict(???)))\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "ns = [10, 20, 50, 100, 150, 200, 300, 400]\n",
    "score_list = [get_avg_score(n) for n in ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.plot(ns, score_list)\n",
    "ax.set(xlabel='Number of Dimensions',\n",
    "       ylabel='Average Accuracy',\n",
    "       title='LogisticRegression Accuracy vs Number of dimensions on the Human Activity Dataset')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
