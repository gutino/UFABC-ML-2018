{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularização e Gradiente Descendente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Vamos iniciar essa lista com um pequeno tutorial sobre regressão, atributos polinomiais e regularização em uma base de dados bem simples contendo uma coluna de variáveis `x` e um valor `y` associado. A base de dados é chamada de `X_Y_Sinusoid_Data.csv`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Exercício 1\n",
    "\n",
    "* Importe a base de dados. \n",
    "* Gere aproximadamente  100 pontos x na faixa de 0 a 1 utilizando a função `linspace` do `numpy`. Usando esses pontos, calcule o ponto y representando o valor verdadeiro a partir da equação: $y = sin(2\\pi x)$\n",
    "\n",
    "* Plote a base e `x` vs `y` da base e o gerado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filepath = 'data/X_Y_Sinusoid_Data.csv'\n",
    "data = pd.read_csv(filepath)\n",
    "\n",
    "X_real = ???\n",
    "Y_real = ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T19:32:58.021116Z",
     "start_time": "2017-03-10T14:32:58.015025-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T19:32:58.921638Z",
     "start_time": "2017-03-10T14:32:58.668630-05:00"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "# Plot of the noisy (sparse)\n",
    "ax = data.set_index('x')['y'].plot(ls='', marker='o', label='base de dados')\n",
    "ax.plot(X_real, Y_real, ls='--', marker='', label='função real')\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlabel='x data', ylabel='y data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2\n",
    "\n",
    "\n",
    "* Usando a classe `PolynomialFeatures` da biblioteca de pré-processamento do Scikit-learn's, crie atributos polinomiais de ordem 20.\n",
    "* Crie um modelo de regressão linear. \n",
    "* Plote o valor predito com o valor calculado.\n",
    "\n",
    "Note que `PolynomialFeatures` requer um dataframe com 1 coluna ou uma array bidimensional de dimensão (`n`, 1), com `n` sendo o número de amostras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T19:57:23.344020Z",
     "start_time": "2017-03-10T14:57:23.057905-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Setup the polynomial features\n",
    "degree = 20\n",
    "pf = PolynomialFeatures(degree)\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Extraindo x e y da base de dados no formato correto\n",
    "X_data = data[['x']]\n",
    "Y_data = data['y']\n",
    "\n",
    "# Create the features and fit the model\n",
    "X_poly = ???\n",
    "lr.fit(???)\n",
    "Y_pred = ???\n",
    "\n",
    "# Plot the result\n",
    "plt.plot(X_data, Y_data, marker='o', ls='', label='base de dados', alpha=1)\n",
    "plt.plot(X_real, Y_real, ls='--', label='função real')\n",
    "plt.plot(X_data, Y_pred, marker='^', alpha=.5, label='predições com atributos polinomiais')\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "ax.set(xlabel='x data', ylabel='y data');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3\n",
    "\n",
    "* Repita o experimento anterior utilizando ridge regression ($\\alpha$=0.001) e lasso regression ($\\alpha$=0.0001). \n",
    "* Plote os resultados.\n",
    "* Em seguida vamos plotar a magnitude dos coefientes dos modelos para comparação.\n",
    "\n",
    "O que a diferença entre as magnitudes te diz sobre a regularização?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T20:11:47.526408Z",
     "start_time": "2017-03-10T15:11:47.216623-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mute the sklearn warning about regularization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='sklearn')\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Crie o modelo Ridge\n",
    "rr = ???\n",
    "rr.fit(???)\n",
    "Y_pred_rr = ???\n",
    "\n",
    "# Crie o modelo Lasso\n",
    "lassor = ???\n",
    "lassor.fit(???)\n",
    "Y_pred_lr = ???\n",
    "\n",
    "# Vamos plotar as predições\n",
    "plt.plot(X_data, Y_data, marker='o', ls='', label='base de dados')\n",
    "plt.plot(X_real, Y_real, ls='--', label='função real')\n",
    "plt.plot(X_data, Y_pred, label='regressão linear', marker='^', alpha=.5)\n",
    "plt.plot(X_data, Y_pred_rr, label='regressão ridge', marker='^', alpha=.5)\n",
    "plt.plot(X_data, Y_pred_lr, label='regressão lasso', marker='^', alpha=.5)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set(xlabel='x data', ylabel='y data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos examinar os coeficientes gerados pelos modelos\n",
    "# .ravel() retorna uma array multidimensional concatenada em uma dimensão (flattened)\n",
    "\n",
    "coefficients = pd.DataFrame()\n",
    "coefficients['linear regression'] = lr.coef_.ravel()\n",
    "coefficients['ridge regression'] = rr.coef_.ravel()\n",
    "coefficients['lasso regression'] = lassor.coef_.ravel()\n",
    "coefficients = coefficients.applymap(abs)\n",
    "\n",
    "coefficients.describe()  # Diferença enorme da escala da regressão linear e das regularizações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T20:39:37.722464Z",
     "start_time": "2017-03-10T15:39:37.347911-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette()\n",
    "\n",
    "# Vamos criar dois eixos, um para regressão linear outro para as regularizações\n",
    "ax1 = plt.axes()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plotar a regressão linear\n",
    "ax1.plot(lr.coef_.ravel(), \n",
    "         color=colors[0], marker='o', label='linear regression')\n",
    "\n",
    "# Plotar as regularizações no outro eixo\n",
    "ax2.plot(rr.coef_.ravel(), \n",
    "         color=colors[1], marker='o', label='ridge regression')\n",
    "\n",
    "ax2.plot(lassor.coef_.ravel(), \n",
    "         color=colors[2], marker='o', label='lasso regression')\n",
    "\n",
    "# Alterar as escalas\n",
    "ax1.set_ylim(-2e14, 2e14)\n",
    "ax2.set_ylim(-25, 25)\n",
    "\n",
    "# Combinar as legendas\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(h1+h2, l1+l2)\n",
    "\n",
    "ax1.set(xlabel='coefficients',ylabel='linear regression')\n",
    "ax2.set(ylabel='ridge and lasso regression')\n",
    "\n",
    "ax1.set_xticks(range(len(lr.coef_)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 4\n",
    "\n",
    "Para os próximos exercícios vamos utilizar a base de dados da atividade anterior sobre o preço dos imóveis.\n",
    "\n",
    "Para começar:\n",
    "\n",
    "* Importe os dados com Pandas e aplique o one-hot-encoding nas variáveis categóricas, vamos utilizar o método `.get_dummies`. \n",
    "* Divida a base entre treino e teste. \n",
    "* Aplique a função de transformação Log nos atributos com viés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:04.174800Z",
     "start_time": "2017-03-10T12:01:04.142735-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = 'data/Ames_Housing_Sales.csv'\n",
    "data1 = pd.read_csv(filepath, sep=',')\n",
    "data = data1.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T02:40:49.956043Z",
     "start_time": "2017-03-09T21:40:49.950878-05:00"
    }
   },
   "source": [
    "Crie uma lista de atributos categóricos e aplique o método `get_dummies` para gerar os atributos one-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:05.304547Z",
     "start_time": "2017-03-10T12:01:05.231567-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a Pd.Series consisting of all the string categoricals\n",
    "one_hot_encode_cols = data.dtypes[data.dtypes == np.object]  # filtering by string categoricals\n",
    "one_hot_encode_cols = one_hot_encode_cols.index.tolist()  # list of categorical fields\n",
    "\n",
    "# Primeiro devemos marcar cada coluna como categórico com pd.Categorical()\n",
    "for col in one_hot_encode_cols:\n",
    "    data[col] = pd.Categorical(data[col])\n",
    "\n",
    "# Agora aplicamos o método get_dummies() nas nossas colunas categóricas\n",
    "data = pd.get_dummies(data, columns=???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos dividir a base entre treino e teste, use `test_size=0.3` e `random_state=42`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:06.260979Z",
     "start_time": "2017-03-10T12:01:06.244259-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns atributos possuem valores enviesados--uma transformação log pode ajudar a aliviar essa situação. Vamos alterar esses atributos, exceto pelo `SalePrice` que é nossa variável alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of float colums to check for skewing\n",
    "mask = data.dtypes == np.float\n",
    "float_cols = data.columns[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:10.689590Z",
     "start_time": "2017-03-10T12:01:10.609841-05:00"
    }
   },
   "outputs": [],
   "source": [
    "skew_limit = 0.75\n",
    "skew_vals = train[float_cols].skew()\n",
    "\n",
    "skew_cols = (skew_vals\n",
    "             .sort_values(ascending=False)\n",
    "             .to_frame()\n",
    "             .rename(columns={0:'Skew'})\n",
    "             .query('abs(Skew) > {0}'.format(skew_limit)))\n",
    "\n",
    "skew_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform all the columns where the skew is greater than 0.75, excluding \"SalePrice\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: Let's look at what happens to one of these features, when we apply np.log1p visually.\n",
    "\n",
    "field = \"BsmtFinSF1\"\n",
    "fig, (ax_before, ax_after) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "train[field].hist(ax=ax_before)\n",
    "train[field].apply(np.log1p).hist(ax=ax_after)\n",
    "ax_before.set(title='before np.log1p', ylabel='frequency', xlabel='value')\n",
    "ax_after.set(title='after np.log1p', ylabel='frequency', xlabel='value')\n",
    "fig.suptitle('Field \"{}\"'.format(field));\n",
    "# a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:15.654621Z",
     "start_time": "2017-03-10T12:01:13.780771-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mute the setting wtih a copy warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "for col in skew_cols.index.tolist():\n",
    "    if col == \"SalePrice\":\n",
    "        continue\n",
    "    # aplique a função np.log1p nessa variável\n",
    "    train[col] = ???\n",
    "    test[col]  = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos separar os atributos do preditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:21.972625Z",
     "start_time": "2017-03-10T12:01:21.957050-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_cols = [x for x in train.columns if x != 'SalePrice']\n",
    "X_train = ???\n",
    "y_train = ???\n",
    "\n",
    "X_test  = ???\n",
    "y_test  = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:11:03.256453",
     "start_time": "2017-02-21T09:11:03.241117"
    }
   },
   "source": [
    "## Exercício 5\n",
    "\n",
    "* Escreva a função **`rmse`**  que recebe valores reais da variável alvo e os peditos pelo modelo e retorna a raiz quadrada do erro quadrático médio. Use a função `mean_squared_error` do sklearn.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(ytrue, ypredicted):\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aplique um modelo básico de regressão linear\n",
    "* imprima o rmse do modelo\n",
    "* plot os valores preditos vs reais baseado no modelo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linearRegression = ???\n",
    "\n",
    "linearRegression_rmse = rmse(???)\n",
    "\n",
    "print(linearRegression_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(6,6))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot(y_test, linearRegression.predict(X_test), \n",
    "         marker='o', ls='', ms=3.0)\n",
    "\n",
    "lim = (0, y_test.max())\n",
    "\n",
    "ax.set(xlabel='Actual Price', \n",
    "       ylabel='Predicted Price', \n",
    "       xlim=lim,\n",
    "       ylim=lim,\n",
    "       title='Linear Regression Results');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 6\n",
    "\n",
    "A regressão Ridge usa a regularização do tipo L2 para reduzir a magnitude dos coeficientes. Isso é especialmente útil quando temos uma variância muito alta. O Scikit-Learn possui esse modelo na classe `Ridge` e uma versão que aplica validação cruzada para determinar o melhor valor de $\\alpha$ na classe `RidgeCV`.\n",
    "\n",
    "* Utilize a classe `Ridge` e determine o melhor valor de $\\alpha$ dentre os valores $$[0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]$$ e plote o RMSE vs $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora repita o experimento utilizando RidgeCV, note que esse modelo retorna apenas o melhor $\\alpha$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:48:27.914740",
     "start_time": "2017-02-21T09:48:27.293957"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = [0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]\n",
    "\n",
    "ridgeCV = RidgeCV(alphas=alphas, \n",
    "                  cv=4).fit(???)\n",
    "\n",
    "ridgeCV_rmse = rmse(???)\n",
    "\n",
    "print(ridgeCV.alpha_, ridgeCV_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 7\n",
    "\n",
    "Agora utilizaremos o `LassoCV` para aplicar a regularização L1 que tenta reduzir o máximo de coeficientes para zero, fazendo uma espécie de seleção de atributos.\n",
    "\n",
    "Além disso, temos a classe `ElasticNetCV` que combina as regularizações L1 e L2.\n",
    "\n",
    "* Use o `LassoCV` e determine o $\\alpha$ ótimo e o RMSE do modelo gerado. \n",
    "* Repita com o modelo Elastic Net\n",
    "* Compare os resultados com tabelas ou plotagens\n",
    "\n",
    "Use os $\\alpha$:  \n",
    "`[1e-5, 5e-5, 0.0001, 0.0005]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:50:10.797247",
     "start_time": "2017-02-21T09:50:09.006978"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "alphas2 = np.array([1e-5, 5e-5, 0.0001, 0.0005])\n",
    "\n",
    "lassoCV = LassoCV(alphas=alphas2,\n",
    "                  max_iter=5e4,\n",
    "                  cv=3).fit(???)\n",
    "\n",
    "lassoCV_rmse = rmse(???)\n",
    "\n",
    "print(lassoCV.alpha_, lassoCV_rmse) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar quantos coeficientes são iguais a zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:50:13.053851",
     "start_time": "2017-02-21T09:50:13.047466"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Of {} coefficients, {} are non-zero with Lasso.'.format(len(lassoCV.coef_), \n",
    "                                                               len(lassoCV.coef_.nonzero()[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T12:03:06.013488",
     "start_time": "2017-02-16T12:03:06.007159"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "Agora teste o elastic net com os mesmos alphas de Lasso e l1_ratios entre 0.1 e 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:51:07.592747",
     "start_time": "2017-02-21T09:50:38.683133"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "l1_ratios = np.linspace(0.1, 0.9, 9)\n",
    "\n",
    "elasticNetCV = ElasticNetCV(alphas=alphas2, \n",
    "                            l1_ratio=l1_ratios,\n",
    "                            max_iter=1e4).fit(???)\n",
    "elasticNetCV_rmse = rmse(???)\n",
    "\n",
    "print(elasticNetCV.alpha_, elasticNetCV.l1_ratio_, elasticNetCV_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos comparar os resultados com uma tabela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:52:29.062678",
     "start_time": "2017-02-21T09:52:28.998572"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse_vals = [linearRegression_rmse, ridgeCV_rmse, lassoCV_rmse, elasticNetCV_rmse]\n",
    "\n",
    "labels = ['Linear', 'Ridge', 'Lasso', 'ElasticNet']\n",
    "\n",
    "rmse_df = pd.Series(???, index=???).to_frame()\n",
    "rmse_df.rename(columns={0: 'RMSE'}, inplace=1)\n",
    "rmse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente podemos fazer um plot do valor real vs predito para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:53:54.142116",
     "start_time": "2017-02-21T09:53:53.857081"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(6,6))\n",
    "ax = plt.axes()\n",
    "\n",
    "labels = ['Ridge', 'Lasso', 'ElasticNet']\n",
    "\n",
    "models = [ridgeCV, lassoCV, elasticNetCV]\n",
    "\n",
    "for mod, lab in zip(models, labels):\n",
    "    ax.plot(y_test, mod.predict(X_test), \n",
    "             marker='o', ls='', ms=3.0, label=lab)\n",
    "\n",
    "\n",
    "leg = plt.legend(frameon=True)\n",
    "leg.get_frame().set_edgecolor('black')\n",
    "leg.get_frame().set_linewidth(1.0)\n",
    "\n",
    "ax.set(xlabel='Actual Price', \n",
    "       ylabel='Predicted Price', \n",
    "       title='Linear Regression Results');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
